{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f1d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import face_recognition\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d9ed0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepika', 'mammootty', 'mohanlal', 'pritviraj', 'sobhana']\n"
     ]
    }
   ],
   "source": [
    "path=r'C:\\Users\\fidha\\OneDrive\\Desktop\\LUMINAR\\DATASETS\\Face Recognition\\my_images'\n",
    "lis=os.listdir(path)\n",
    "images=[]\n",
    "labels=[]\n",
    "# print(lis)\n",
    "for i in lis:\n",
    "  img=cv2.imread(os.path.join(path,i)) #path should be given because images are not directly accessible naa?..they \n",
    "  # are inside the folder my_images.\n",
    "  images.append(img)\n",
    "  p=os.path.splitext(i)# os.path.splitext is a Python function that splits a file path into two parts:\n",
    "# the file name (or path) without the extension\n",
    "# the file extension\n",
    "  labels.append(p[0])\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a07d32db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(images))\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf44746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        ...,\n",
      "        [ 55,  42,  50],\n",
      "        [ 54,  41,  49],\n",
      "        [ 53,  40,  48]],\n",
      "\n",
      "       [[  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        ...,\n",
      "        [ 54,  41,  49],\n",
      "        [ 54,  41,  49],\n",
      "        [ 53,  40,  48]],\n",
      "\n",
      "       [[  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        [  7,   2,   1],\n",
      "        ...,\n",
      "        [ 54,  41,  49],\n",
      "        [ 53,  40,  48],\n",
      "        [ 53,  40,  48]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 64,  75, 255],\n",
      "        [ 64,  75, 255],\n",
      "        [ 63,  74, 255],\n",
      "        ...,\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253]],\n",
      "\n",
      "       [[ 64,  74, 255],\n",
      "        [ 64,  74, 255],\n",
      "        [ 63,  73, 255],\n",
      "        ...,\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253]],\n",
      "\n",
      "       [[ 64,  74, 255],\n",
      "        [ 64,  74, 255],\n",
      "        [ 63,  73, 255],\n",
      "        ...,\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253],\n",
      "        [ 65,  63, 253]]], shape=(275, 183, 3), dtype=uint8), array([[[247, 252, 251],\n",
      "        [246, 251, 250],\n",
      "        [245, 250, 249],\n",
      "        ...,\n",
      "        [251, 253, 253],\n",
      "        [252, 254, 255],\n",
      "        [252, 254, 255]],\n",
      "\n",
      "       [[250, 255, 254],\n",
      "        [249, 254, 253],\n",
      "        [247, 252, 251],\n",
      "        ...,\n",
      "        [251, 253, 253],\n",
      "        [251, 253, 254],\n",
      "        [252, 254, 255]],\n",
      "\n",
      "       [[250, 255, 254],\n",
      "        [249, 254, 253],\n",
      "        [248, 253, 252],\n",
      "        ...,\n",
      "        [251, 253, 253],\n",
      "        [251, 253, 254],\n",
      "        [251, 253, 254]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[213, 217, 218],\n",
      "        [212, 216, 217],\n",
      "        [211, 215, 216],\n",
      "        ...,\n",
      "        [243, 246, 230],\n",
      "        [255, 255, 249],\n",
      "        [253, 255, 249]],\n",
      "\n",
      "       [[213, 217, 218],\n",
      "        [212, 216, 217],\n",
      "        [211, 215, 216],\n",
      "        ...,\n",
      "        [213, 214, 198],\n",
      "        [255, 255, 247],\n",
      "        [255, 255, 252]],\n",
      "\n",
      "       [[213, 217, 218],\n",
      "        [212, 216, 217],\n",
      "        [211, 215, 216],\n",
      "        ...,\n",
      "        [178, 178, 162],\n",
      "        [243, 241, 233],\n",
      "        [255, 254, 250]]], shape=(204, 204, 3), dtype=uint8), array([[[218, 205, 197],\n",
      "        [195, 180, 171],\n",
      "        [168, 150, 139],\n",
      "        ...,\n",
      "        [217, 225, 214],\n",
      "        [217, 225, 214],\n",
      "        [216, 222, 211]],\n",
      "\n",
      "       [[220, 207, 199],\n",
      "        [201, 186, 177],\n",
      "        [178, 160, 149],\n",
      "        ...,\n",
      "        [217, 225, 214],\n",
      "        [217, 225, 214],\n",
      "        [216, 222, 211]],\n",
      "\n",
      "       [[221, 209, 199],\n",
      "        [210, 196, 184],\n",
      "        [192, 175, 162],\n",
      "        ...,\n",
      "        [217, 225, 214],\n",
      "        [217, 225, 214],\n",
      "        [216, 222, 211]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[176, 181, 179],\n",
      "        [176, 181, 179],\n",
      "        [177, 182, 180],\n",
      "        ...,\n",
      "        [229, 234, 232],\n",
      "        [229, 234, 232],\n",
      "        [232, 238, 233]],\n",
      "\n",
      "       [[176, 181, 179],\n",
      "        [176, 181, 179],\n",
      "        [177, 182, 180],\n",
      "        ...,\n",
      "        [229, 235, 230],\n",
      "        [229, 235, 230],\n",
      "        [232, 238, 233]],\n",
      "\n",
      "       [[178, 181, 179],\n",
      "        [178, 181, 179],\n",
      "        [179, 182, 180],\n",
      "        ...,\n",
      "        [231, 235, 229],\n",
      "        [231, 235, 229],\n",
      "        [234, 240, 235]]], shape=(225, 225, 3), dtype=uint8), array([[[ 34,  32, 108],\n",
      "        [ 36,  29, 120],\n",
      "        [ 39,  28, 132],\n",
      "        ...,\n",
      "        [ 61,  44, 207],\n",
      "        [ 56,  38, 205],\n",
      "        [ 60,  41, 212]],\n",
      "\n",
      "       [[ 34,  32, 108],\n",
      "        [ 36,  29, 120],\n",
      "        [ 39,  28, 132],\n",
      "        ...,\n",
      "        [ 61,  44, 207],\n",
      "        [ 55,  37, 204],\n",
      "        [ 59,  40, 211]],\n",
      "\n",
      "       [[ 34,  32, 108],\n",
      "        [ 36,  29, 120],\n",
      "        [ 39,  28, 132],\n",
      "        ...,\n",
      "        [ 60,  43, 206],\n",
      "        [ 55,  37, 204],\n",
      "        [ 59,  40, 211]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 54,  42, 184],\n",
      "        [ 53,  41, 183],\n",
      "        [ 53,  40, 180],\n",
      "        ...,\n",
      "        [ 47,  35, 161],\n",
      "        [ 47,  34, 162],\n",
      "        [ 48,  35, 163]],\n",
      "\n",
      "       [[ 54,  42, 184],\n",
      "        [ 53,  41, 183],\n",
      "        [ 53,  40, 180],\n",
      "        ...,\n",
      "        [ 47,  35, 161],\n",
      "        [ 47,  34, 162],\n",
      "        [ 48,  35, 163]],\n",
      "\n",
      "       [[ 54,  42, 184],\n",
      "        [ 53,  41, 183],\n",
      "        [ 53,  40, 180],\n",
      "        ...,\n",
      "        [ 47,  35, 161],\n",
      "        [ 47,  34, 162],\n",
      "        [ 48,  35, 163]]], shape=(159, 318, 3), dtype=uint8), array([[[ 71,  97, 109],\n",
      "        [ 71,  97, 109],\n",
      "        [ 72,  98, 110],\n",
      "        ...,\n",
      "        [111, 133, 139],\n",
      "        [111, 133, 138],\n",
      "        [110, 132, 137]],\n",
      "\n",
      "       [[ 72,  98, 110],\n",
      "        [ 72,  98, 110],\n",
      "        [ 73,  99, 111],\n",
      "        ...,\n",
      "        [112, 134, 140],\n",
      "        [111, 133, 138],\n",
      "        [111, 133, 138]],\n",
      "\n",
      "       [[ 74, 100, 112],\n",
      "        [ 74, 100, 112],\n",
      "        [ 74, 100, 112],\n",
      "        ...,\n",
      "        [113, 135, 141],\n",
      "        [112, 134, 139],\n",
      "        [112, 134, 139]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        ...,\n",
      "        [ 32,  49,  52],\n",
      "        [ 25,  43,  44],\n",
      "        [ 19,  37,  38]],\n",
      "\n",
      "       [[  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        ...,\n",
      "        [ 28,  46,  47],\n",
      "        [ 23,  38,  40],\n",
      "        [ 17,  32,  34]],\n",
      "\n",
      "       [[  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        [  9,  18,  15],\n",
      "        ...,\n",
      "        [ 25,  43,  44],\n",
      "        [ 20,  35,  37],\n",
      "        [ 13,  28,  30]]], shape=(268, 188, 3), dtype=uint8)]\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd89dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgpath=r\"C:\\Users\\fidha\\OneDrive\\Desktop\\LUMINAR\\DATASETS\\Face Recognition\\sachin1.jpeg\"\n",
    "# img=cv2.imread(imgpath)\n",
    "# cv2.imshow(\"w\",img)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "# print(img is None)\n",
    "# img1=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "# print(img1 is None )\n",
    "# faceloc=face_recognition.face_locations(img1) \n",
    "# faceloc\n",
    "# face_encoded=face_recognition.face_encodings(img,faceloc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95271e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m         encoded_list.append(face_encoded)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded_list\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m encolist=\u001b[43mencod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mencod\u001b[39m\u001b[34m(images)\u001b[39m\n\u001b[32m      2\u001b[39m encoded_list=[]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# img=cv2.cvtColor(i,cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# img = np.ascontiguousarray(img, dtype=np.uint8)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     faceloc=\u001b[43mface_recognition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#face_locations take rgb images\u001b[39;00m\n\u001b[32m      7\u001b[39m     face_encoded=face_recognition.face_encodings(img,faceloc)[\u001b[32m0\u001b[39m] \u001b[38;5;66;03m#face_encodings return a list of arrays..where each array\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# represents the encoding of a single detected face in the image. here [0] is used to access the first(and only) encoded face from that list\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# since we r having only one face in each image. \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fidha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:121\u001b[39m, in \u001b[36mface_locations\u001b[39m\u001b[34m(img, number_of_times_to_upsample, model)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001b[33m\"\u001b[39m\u001b[33mcnn\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face), img.shape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fidha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:105\u001b[39m, in \u001b[36m_raw_face_locations\u001b[39m\u001b[34m(img, number_of_times_to_upsample, model)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "def encod(images):\n",
    "    encoded_list=[]\n",
    "    for i in images:\n",
    "        img=cv2.cvtColor(i,cv2.COLOR_BGR2RGB)\n",
    "        # img = np.ascontiguousarray(img, dtype=np.uint8)\n",
    "        faceloc=face_recognition.face_locations(img)  #face_locations take rgb images\n",
    "        face_encoded=face_recognition.face_encodings(img,faceloc)[0] #face_encodings return a list of arrays..where each array\n",
    "        # represents the encoding of a single detected face in the image. here [0] is used to access the first(and only) encoded face from that list\n",
    "        # since we r having only one face in each image. \n",
    "        #through face encoding we r converting the face into a numerical representation to which we compare the new faces.\n",
    "        encoded_list.append(face_encoded)\n",
    "    return encoded_list\n",
    "encolist=encod(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336b5ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Detect face locations in the frame\u001b[39;00m\n\u001b[32m      9\u001b[39m img1=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m face_in_frame = \u001b[43mface_recognition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m face_encodings = face_recognition.face_encodings(img1, face_in_frame)\u001b[38;5;66;03m#[0] is not given here because there can be multiple faces\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# in the frame.We want all of them to be detected.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fidha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:121\u001b[39m, in \u001b[36mface_locations\u001b[39m\u001b[34m(img, number_of_times_to_upsample, model)\u001b[39m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m _raw_face_locations(img, number_of_times_to_upsample, \u001b[33m\"\u001b[39m\u001b[33mcnn\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [_trim_css_to_bounds(_rect_to_css(face), img.shape) \u001b[38;5;28;01mfor\u001b[39;00m face \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_raw_face_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\fidha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:105\u001b[39m, in \u001b[36m_raw_face_locations\u001b[39m\u001b[34m(img, number_of_times_to_upsample, model)\u001b[39m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cnn_face_detector(img, number_of_times_to_upsample)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mface_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_times_to_upsample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = video.read()\n",
    "    if not success:\n",
    "        break\n",
    "    img1=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)  \n",
    "    face_in_frame = face_recognition.face_locations(img1) #face_locations also returns a list of tuples of face locations in the image.\n",
    "    face_encodings = face_recognition.face_encodings(img1, face_in_frame)#[0] is not given here because there can be multiple faces\n",
    "    # in the frame.We want all of them to be detected.\n",
    "    # so now both face_encodings and face in_frame are lists\n",
    "    for en_face,loc in zip(face_encodings,face_in_frame):\n",
    "        matches=face_recognition.compare_faces(encolist,en_face)\n",
    "        # compare_faces compares a list of known face encodings against a candidate face encoding to see if they match.\n",
    "        # it returns a list of boolean values indicating whether each known face matches the candidate face.\n",
    "        print(matches)\n",
    "        faceddistance=face_recognition.face_distance(encolist,en_face)\n",
    "        # face_distance calculates the Euclidean distance between a candidate face encoding and a list of known face encodings.\n",
    "        # it returns an array of distances, where smaller distances indicate greater similarity.\n",
    "        f=np.argmin(faceddistance)  #np.argmin returns the indices of the minimum values along an axis.\n",
    "        print(faceddistance)\n",
    "        if matches[f]:\n",
    "            label=labels[f]\n",
    "            print(label)\n",
    "        # the order of the cordinates that face_locations returns is y1,x2,y2,x1.\n",
    "        # so to draw the rectangle\n",
    "           y1,x2,y2,x1=loc\n",
    "           cv2.rectangle(img,pt1=(x1,y1),pt2=(x2,y2),color=(0,255,0),thickness=2)\n",
    "           cv2.putText(img,label,(x1,y1),cv2.FONT_HERSHEY_SIMPLEX,0.9,(0,255,0),2)\n",
    "\n",
    "    cv2.imshow(\"image\", img)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
